{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d8b734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\multi\\AppData\\Roaming\\Python\\Python313\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.13.0)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pygame setup\n",
    "pygame.init()\n",
    "GRID_SIZE = 20\n",
    "SCREEN_WIDTH = 400\n",
    "SCREEN_HEIGHT = 450  # Extra space for score\n",
    "screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "pygame.display.set_caption(\"Snake DQN\")\n",
    "clock = pygame.time.Clock()\n",
    "font = pygame.font.SysFont('Arial', 24)\n",
    "\n",
    "# DQN parameters\n",
    "STATE_SIZE = 9  # Simplified state\n",
    "HIDDEN_SIZE = 128\n",
    "ACTION_SPACE = 3  # left, straight, right\n",
    "BATCH_SIZE = 128\n",
    "MEMORY_SIZE = 100_000\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "BG_COLOR = (0, 128, 0)  \n",
    "SNAKE_COLOR = (52, 21, 57)  \n",
    "FOOD_COLOR = (255, 0, 0)  \n",
    "SCOREBOARD_COLOR = (0, 0, 0)  \n",
    "GRID_LINE_COLOR = (50, 50, 50)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a84d4715",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "\n",
    "    def __init__(self, STATE_SIZE, HIDDEN_SIZE, ACTION_SPACE):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(STATE_SIZE, HIDDEN_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_SIZE, ACTION_SPACE)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)   \n",
    "\n",
    "    def save(self, file_name='model.pth'):\n",
    "        model_folder_path = './model'\n",
    "        if not os.path.exists(model_folder_path):\n",
    "            os.makedirs(model_folder_path)\n",
    "\n",
    "        file_name = os.path.join(model_folder_path, file_name)\n",
    "        torch.save(self.state_dict(), file_name)\n",
    "\n",
    "class TrainerDQN:\n",
    "    def __init__(self, model, lr, discount):\n",
    "        self.lr = lr\n",
    "        self.gamma = discount\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def train_step(self, state, action, reward, next_state, done):\n",
    "        state = torch.tensor(state, dtype=torch.float)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "        action = torch.tensor(action, dtype=torch.long)\n",
    "        reward = torch.tensor(reward, dtype=torch.float)\n",
    "\n",
    "        if len(state.shape) == 1:\n",
    "            state = torch.unsqueeze(state, 0)\n",
    "            next_state = torch.unsqueeze(next_state, 0)\n",
    "            action = torch.unsqueeze(action, 0)\n",
    "            reward = torch.unsqueeze(reward, 0)\n",
    "            done = (done, )\n",
    "#this is done for long term shorterm memory, so we can use the same code for both single and batch training\n",
    "\n",
    "        pred = self.model(state)\n",
    "\n",
    "        dummy = pred.clone()\n",
    "\n",
    "        for i in range(len(done)):\n",
    "            Q_new = reward[i]\n",
    "            if not done[i]:\n",
    "                Q_new = reward[i] + self.gamma * torch.max(self.model(next_state[i]))\n",
    "\n",
    "\n",
    "            dummy[i][action[i].item()] = Q_new\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(dummy, pred)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        #standard pytorch training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79bc54d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SnakeGameDQN:\n",
    "    def __init__(self):\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "        pygame.display.set_caption(\"Q-Learning Snake\")\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.font = pygame.font.SysFont('Arial', 20) # Using 'Arial' for consistency\n",
    "        self.done = False\n",
    "        self.reset_game()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def draw_game(self, screen, snake, food, score):\n",
    "        screen.fill(BG_COLOR)\n",
    "        \n",
    "        # Scoreboard\n",
    "        pygame.draw.rect(screen, SCOREBOARD_COLOR, (0, 0, SCREEN_WIDTH, 50))\n",
    "        pygame.draw.line(screen, (0, 0, 0), (0, 50), (SCREEN_WIDTH, 50), 2)\n",
    "        \n",
    "        # Score text\n",
    "        font = pygame.font.SysFont('freesanbold.ttf', 40)\n",
    "        text = font.render(f\"Score: {score}\", True, (255, 255, 255))\n",
    "        text_rect = text.get_rect(center=(SCREEN_WIDTH//2, 25))\n",
    "        screen.blit(text, text_rect)\n",
    "        \n",
    "        # Grid\n",
    "        for x in range(0, SCREEN_WIDTH, GRID_SIZE):\n",
    "            pygame.draw.line(screen, GRID_LINE_COLOR, (x, 50), (x, SCREEN_HEIGHT))\n",
    "        for y in range(50, SCREEN_HEIGHT, GRID_SIZE):\n",
    "            pygame.draw.line(screen, GRID_LINE_COLOR, (0, y), (SCREEN_WIDTH, y))\n",
    "        \n",
    "        # Snake\n",
    "        for segment in self.snake:\n",
    "            rect = pygame.Rect(segment[0]*GRID_SIZE, segment[1]*GRID_SIZE + 50, \n",
    "                            GRID_SIZE, GRID_SIZE)\n",
    "            pygame.draw.rect(screen, SNAKE_COLOR, rect, 0) \n",
    "        \n",
    "        # Food\n",
    "        food_rect = pygame.Rect(food[0]*GRID_SIZE, food[1]*GRID_SIZE + 50, \n",
    "                            GRID_SIZE, GRID_SIZE)\n",
    "        pygame.draw.circle(screen, FOOD_COLOR, food_rect.center, 10)\n",
    "        \n",
    "        pygame.display.update()\n",
    "\n",
    "    def get_state(self):\n",
    "        head = self.snake[0]\n",
    "        \n",
    "        # Food relative position\n",
    "        food_dx = np.sign(self.food[0] - head[0])\n",
    "        food_dy = np.sign(self.food[1] - head[1])\n",
    "        \n",
    "        # Danger detection\n",
    "        directions = {\n",
    "            'left': (-1, 0) if self.direction == (0, -1) else \n",
    "                    (1, 0) if self.direction == (0, 1) else \n",
    "                    (0, 1) if self.direction == (1, 0) else \n",
    "                    (0, -1),\n",
    "            'forward': self.direction,\n",
    "            'right': (1, 0) if self.direction == (0, -1) else \n",
    "                    (-1, 0) if self.direction == (0, 1) else \n",
    "                    (0, -1) if self.direction == (1, 0) else \n",
    "                    (0, 1)\n",
    "        }\n",
    "        \n",
    "        dangers = []\n",
    "        for d in ['left', 'forward', 'right']:\n",
    "            new_pos = (head[0] + directions[d][0], head[1] + directions[d][1])\n",
    "            # Check boundaries and self-collision\n",
    "            if (new_pos[0] < 0 or new_pos[0] >= GRID_SIZE or \n",
    "                new_pos[1] < 0 or new_pos[1] >= GRID_SIZE or \n",
    "                new_pos in self.snake):\n",
    "                dangers.append(1)\n",
    "            else:\n",
    "                dangers.append(0)\n",
    "        \n",
    "        # Current direction (one-hot encoded)\n",
    "        dir_one_hot = [0, 0, 0, 0]\n",
    "        if self.direction == (1, 0): dir_one_hot[0] = 1    # right\n",
    "        elif self.direction == (-1, 0): dir_one_hot[1] = 1 # left\n",
    "        elif self.direction == (0, 1): dir_one_hot[2] = 1  # down\n",
    "        else: dir_one_hot[3] = 1                           # up\n",
    "        \n",
    "        state = np.array([\n",
    "            food_dx, food_dy,          # Food direction (2)\n",
    "            *dangers,                  # Danger left, front, right (3)\n",
    "            *dir_one_hot               # Current direction one-hot (4)\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        if state is None:\n",
    "            raise ValueError(\"State is None in get_state(). Check game logic for state generation.\")\n",
    "        return state\n",
    "\n",
    "\n",
    "    def get_direction_from_action(self, action, current_direction):\n",
    "        # Action: 0=left, 1=forward, 2=right\n",
    "        if current_direction == (1, 0):  \n",
    "            if action == 0: return (0, -1)  \n",
    "            elif action == 1: return (1, 0)  \n",
    "            elif action == 2: return (0, 1)  \n",
    "        elif current_direction == (-1, 0): \n",
    "            if action == 0: return (0, 1)  \n",
    "            elif action == 1: return (-1, 0) \n",
    "            elif action == 2: return (0, -1)  \n",
    "        elif current_direction == (0, 1):  \n",
    "            if action == 0: return (1, 0) \n",
    "            elif action == 1: return (0, 1) \n",
    "            elif action == 2: return (-1, 0)  \n",
    "        elif current_direction == (0, -1):  \n",
    "            if action == 0: return (-1, 0) \n",
    "            elif action == 1: return (0, -1)  \n",
    "            elif action == 2: return (1, 0)  \n",
    "        return current_direction\n",
    "\n",
    "\n",
    "    def reset_game(self):\n",
    "        self.snake = [[10, 10], [9, 10], [8, 10]]\n",
    "        self.food = [random.randint(0, GRID_SIZE-1), random.randint(0, GRID_SIZE-1)]\n",
    "        while self.food in self.snake:\n",
    "            self.food = [random.randint(0, GRID_SIZE-1), random.randint(0, GRID_SIZE-1)]\n",
    "        \n",
    "        self.direction = (1, 0)  # Reset to initial direction\n",
    "        self.score = 0           # Reset score\n",
    "        self.steps = 0           # Reset steps\n",
    "        self.done = False        # Reset done flag\n",
    "\n",
    "    def play_step(self, action):\n",
    "        # Get new direction based on action\n",
    "        self.direction = self.get_direction_from_action(action, self.direction)\n",
    "        \n",
    "        # Move snake without wrapping around\n",
    "        head = self.snake[0]\n",
    "        new_head = [head[0] + self.direction[0], head[1] + self.direction[1]]\n",
    "        \n",
    "        # Check for collisions with walls or self\n",
    "        if (new_head[0] < 0 or new_head[0] >= GRID_SIZE or # Wall collision X\n",
    "            new_head[1] < 0 or new_head[1] >= GRID_SIZE or # Wall collision Y\n",
    "            new_head in self.snake): # Self-collision\n",
    "            reward = -100\n",
    "            self.done = True\n",
    "        else:\n",
    "            self.snake.insert(0, new_head)\n",
    "            \n",
    "            if new_head == self.food:\n",
    "                reward = 20\n",
    "                self.score += 10\n",
    "                self.steps = 0\n",
    "                self.food = [random.randint(0, GRID_SIZE-1), random.randint(0, GRID_SIZE-1)]\n",
    "                while self.food in self.snake:\n",
    "                    self.food = [random.randint(0, GRID_SIZE-1), random.randint(0, GRID_SIZE-1)]\n",
    "            else:\n",
    "                reward = 0  \n",
    "                self.snake.pop()\n",
    "            \n",
    "            # Check if stuck (too many steps without eating)\n",
    "            self.steps += 1\n",
    "            if self.steps > 100 * len(self.snake):\n",
    "                reward = -50\n",
    "                self.done = True\n",
    "        \n",
    "        return reward, self.done, self.score\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89249c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.epsilon = 0.9\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.9999\n",
    "        self.learning_rate = LEARNING_RATE\n",
    "        self.discount_factor = 0.99\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.model = LinearModel(STATE_SIZE, HIDDEN_SIZE, ACTION_SPACE)\n",
    "        self.trainer = TrainerDQN(self.model, LEARNING_RATE, self.discount_factor)\n",
    "        self.n_games = 0\n",
    "        # self.episodes = 10000\n",
    "        # self.show_every = 1000\n",
    "        # self.state_size = 8  # Simplified state representation\n",
    "        # self.action_space = 3  # left, straight, right\n",
    "\n",
    "    def get_state(self, game):\n",
    "        return game.get_state()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, 2)\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float)\n",
    "            state = torch.unsqueeze(state, 0) \n",
    "            pred = self.model(state)\n",
    "            action = torch.argmax(pred).item()\n",
    "            return action  \n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train_long_memory(self):\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            mini_sample = random.sample(self.memory, BATCH_SIZE) # list of tuples\n",
    "        else:\n",
    "            mini_sample = self.memory\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*mini_sample)\n",
    "        self.trainer.train_step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def train_short_memory(self, state, action, reward, next_state, done):\n",
    "        self.trainer.train_step(state, action, reward, next_state, done)\n",
    "\n",
    "\n",
    "def train_DQN():\n",
    "    pygame.init()\n",
    "    screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "    pygame.display.set_caption(\"DQN Snake\")\n",
    "    clock = pygame.time.Clock()\n",
    "    \n",
    "    scores = []\n",
    "    moving_avg_scores = []\n",
    "    total_score = 0\n",
    "    record = 0\n",
    "    agent = QLearningAgent()\n",
    "    game = SnakeGameDQN()\n",
    "    \n",
    "    SHOW_EVERY = 500  \n",
    "    \n",
    "    while True:\n",
    "        game.reset_game()\n",
    "        state_old= game.get_state()\n",
    "\n",
    "        while not game.done:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "            \n",
    "\n",
    "            action = agent.get_action(state_old)\n",
    "            \n",
    "            reward, done, score = game.play_step(action)\n",
    "            state_new = game.get_state()\n",
    "            \n",
    "            agent.train_short_memory(state_old, action, reward, state_new, done)\n",
    "            \n",
    "            # Remember\n",
    "            agent.remember(state_old, action, reward, state_new, done)\n",
    "            \n",
    "            state_old = state_new\n",
    "            \n",
    "            # Visualize\n",
    "            if agent.n_games % SHOW_EVERY == 0:\n",
    "                game.draw_game(screen, game.snake, game.food, score)\n",
    "                pygame.display.update()\n",
    "                clock.tick(60)  \n",
    "        \n",
    "       \n",
    "        agent.train_long_memory()\n",
    "        \n",
    "        # Update game count and epsilon\n",
    "        agent.n_games += 1\n",
    "        agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
    "        \n",
    "        # Update scores\n",
    "        scores.append(score)\n",
    "        total_score += score\n",
    "        mean_score = total_score / agent.n_games\n",
    "        moving_avg_scores.append(mean_score)\n",
    "        \n",
    "        # Save model if new record\n",
    "        if score > record:\n",
    "            record = score\n",
    "            agent.model.save()\n",
    "        \n",
    "        print('Game', agent.n_games, 'Score', score, 'Record:', record, 'Epsilon:', agent.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_DQN()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
